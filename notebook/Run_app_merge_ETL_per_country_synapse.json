{
	"name": "Run_app_merge_ETL_per_country_synapse",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "fbsynapsedemosp",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7dd38be0-646d-4235-9e0d-529e00589d89"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/d6240a3c-34a4-4a5c-955b-06228bf34ca8/resourceGroups/fb-synapse-demo-rg/providers/Microsoft.Synapse/workspaces/fb-synapse-demo/bigDataPools/fbsynapsedemosp",
				"name": "fbsynapsedemosp",
				"type": "Spark",
				"endpoint": "https://fb-synapse-demo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/fbsynapsedemosp",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import csv  # noqa: F401\r\n",
					"import json\r\n",
					"import os\r\n",
					"import sys\r\n",
					"from pathlib import Path\r\n",
					"\r\n",
					"import numpy as np  # noqa: F401\r\n",
					"import pandas as pd  # noqa: F401\r\n",
					"from src.transforms.fusion_models_synapse import FusionMerge  # noqa: F401\r\n",
					"from src.transforms.fusion_utils import CSVSource, Data, ParquetSource, save_df\r\n",
					"from src.transforms.transforms_synapse import TCGWILabelsTranform, TCOneHotEncoder\r\n",
					"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
					"from azure.storage.filedatalake import DataLakeFileClient\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"container_name=\"fb-synapse-demo-cr\"\r\n",
					"container_conn_str = \"DefaultEndpointsProtocol=https;AccountName=fbsynapsedemodl;AccountKey=2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q==;EndpointSuffix=core.windows.net\"\r\n",
					"account_name = 'fbsynapsedemodl'\r\n",
					"account_key = '2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='\r\n",
					"file_system_name = 'fb-synapse-demo-cr'\r\n",
					"\r\n",
					"\r\n",
					"def json_reader(filename):\r\n",
					"    print('json_file', filename)\r\n",
					"   \r\n",
					"    blob_service_client = BlobServiceClient.from_connection_string(container_conn_str)\r\n",
					"    container_client = blob_service_client.get_container_client(container_name)\r\n",
					"    blob_client = container_client.get_blob_client(filename)\r\n",
					"    streamdownloader = blob_client.download_blob()\r\n",
					"    fileReader = json.loads(streamdownloader.readall())\r\n",
					"    return (fileReader)\r\n",
					"\r\n",
					"def check_file_exist(filename):\r\n",
					"    blob_service_client = BlobServiceClient.from_connection_string(container_conn_str)\r\n",
					"    container_client = blob_service_client.get_container_client(container_name)\r\n",
					"\r\n",
					"    # Check if the blob (file) exists\r\n",
					"    blob_exists = container_client.get_blob_client(filename).exists()\r\n",
					"\r\n",
					"    if blob_exists:\r\n",
					"        return True\r\n",
					"    else:\r\n",
					"        return False\r\n",
					"\r\n",
					"\r\n",
					"year = '2022'\r\n",
					"\r\n",
					"print(f\"Running TC GWI app preprocessing and merge ETL for {year} data.\")\r\n",
					"\r\n",
					"\r\n",
					"# tc_data_dir = os.path.expanduser('~/DS_pocs/Fusion/TruthEngine/data/raw/Truth Central parquet files/')\r\n",
					"tc_data_dir = \"/Truth Central parquet files/\"\r\n",
					"if year == '2022':\r\n",
					"    tc_resp_fn = 'AllRespondents.parquet'\r\n",
					"    tc_config = '/transforms/TC_'+year+'_lists_and_dicts.json'\r\n",
					"else:\r\n",
					"    raise ValueError(\r\n",
					"        f\"You entered year={year}, which is an invalid option.  Please enter year='2022'.\"\r\n",
					"    )\r\n",
					"\r\n",
					"gwi_config = ('/transforms/GWI_lists_and_dicts_synapse.json')\r\n",
					"\r\n",
					"gwi_config_dict = json_reader(gwi_config)\r\n",
					"tc_config_dict = json_reader(tc_config)\r\n",
					"\r\n",
					"# Vectorise TC data\r\n",
					"\r\n",
					"\r\n",
					"if check_file_exist('TC_onehot/' + f'TC_{year}_raw_format_onehot.parquet'):\r\n",
					"    filename = 'TC_onehot/' + f'TC_{year}_raw_format_onehot.parquet'\r\n",
					"    tc_onehot_df = pd.read_parquet('abfss://fb-synapse-demo-cr@fbsynapsedemodl.dfs.core.windows.net/'+filename\r\n",
					"    ,storage_options={'account_key':'2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='})\r\n",
					"    \r\n",
					"    print(f'TC {year} onehot loaded')\r\n",
					"\r\n",
					"else:\r\n",
					"    tc_onehot = TCOneHotEncoder(tc_data=Data(source=ParquetSource(tc_data_dir, 'AllResultMeta2022.parquet')).run(),\r\n",
					"                                save_mapping=True,\r\n",
					"                                mapping_fpath=os.path.join('Combined_labels/', f'AllResultMeta{year}_question_encodings.json')\r\n",
					"                                )\r\n",
					"\r\n",
					"    tc_onehot_df = tc_onehot.run()\r\n",
					"\r\n",
					"    save_df(tc_onehot_df, 'TC_onehot/', f'TC_{year}_raw_format_onehot', file_type='parquet')\r\n",
					"    print(f'TC {year} onehot saved')\r\n",
					"\r\n",
					"if check_file_exist('Combined_labels/TC_GWI_'+ year+'_combined_labels.parquet'):\r\n",
					"    filename = 'Combined_labels/' + 'TC_GWI_'+ year+'_combined_labels.parquet'\r\n",
					"    combined_labels=pd.read_parquet('abfss://fb-synapse-demo-cr@fbsynapsedemodl.dfs.core.windows.net/'+filename\r\n",
					"    ,storage_options={'account_key':'2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='})\r\n",
					"    print(f'TC GWI {year} combined labels loaded')\r\n",
					"\r\n",
					"else:\r\n",
					"    if check_file_exist('Combined_labels/', f'AllResultMeta'+year+'_question_encodings.json'):\r\n",
					"        mapping_dict = json_reader('Combined_labels/' + f'AllResultMeta'+year+'_question_encodings.json')\r\n",
					"        print(f'TC {year} question encodings loaded')\r\n",
					"\r\n",
					"    else:\r\n",
					"        raise FileNotFoundError(\r\n",
					"            f\"Please run TCOneHotEncoder to generate the mapping file for {year} data.\"\r\n",
					"        )\r\n",
					"\r\n",
					"    labels_config = '/transforms/TC_GWI_labels_lists_and_dicts.json'\r\n",
					"    labels_config_dict = json_reader(labels_config)\r\n",
					"\r\n",
					"    label_transformer = TCGWILabelsTranform(\r\n",
					"        tc_questions=Data(source=ParquetSource(tc_data_dir, 'AllQuestions New.parquet')).run(),\r\n",
					"        tc_short_question=Data(source=ParquetSource(tc_data_dir, 'AllShortQuestions New.parquet')).run(),\r\n",
					"        tc_results=Data(source=ParquetSource(tc_data_dir, 'AllResultMeta2022.parquet')).run(),\r\n",
					"        year=year,\r\n",
					"        config=labels_config_dict,\r\n",
					"        mapping=mapping_dict\r\n",
					"    )\r\n",
					"\r\n",
					"    combined_labels = label_transformer.run()\r\n",
					"\r\n",
					"    save_df(combined_labels,  'Combined_labels/', f'TC_GWI_{year}_combined_labels', file_type='parquet')\r\n",
					"    print(f'TC GWI {year} combined labels saved')\r\n",
					"\r\n",
					"\r\n",
					"# Get mutual countries between GWI and TC for the year\r\n",
					"if check_file_exist('Fusion_ready/' + f'GWI_{year}_fusion_cols_ready_for_fusion_synapse.parquet'):\r\n",
					"    filename = 'Fusion_ready/' + f'GWI_{year}_fusion_cols_ready_for_fusion_synapse.parquet'\r\n",
					"    \r\n",
					"    df_gwi_fusion_ready=pd.read_parquet('abfss://fb-synapse-demo-cr@fbsynapsedemodl.dfs.core.windows.net/'+filename\r\n",
					"    ,storage_options={'account_key':'2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='}, columns=['Country'])\r\n",
					"    gwi_countries = df_gwi_fusion_ready['Country'].unique()\r\n",
					"\r\n",
					"else:\r\n",
					"    raise FileNotFoundError(\r\n",
					"        f\"Please run Fusion ETL to generate the GWI fusion ready data for {year} data.\"\r\n",
					"    )\r\n",
					"\r\n",
					"if check_file_exist( 'Fusion_ready/' + f'TC_{year}_fusion_cols_ready_for_fusion.parquet'):\r\n",
					"    filename = 'Fusion_ready/' + f'TC_{year}_fusion_cols_ready_for_fusion.parquet'\r\n",
					"    df_tc_fusion_ready = pd.read_parquet('abfss://fb-synapse-demo-cr@fbsynapsedemodl.dfs.core.windows.net/'+filename\r\n",
					"    ,storage_options={'account_key':'2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='}, columns=['Country'])\r\n",
					"    tc_countries = df_tc_fusion_ready['Country'].unique()\r\n",
					"\r\n",
					"else:\r\n",
					"    raise FileNotFoundError(\r\n",
					"        \r\n",
					"        f\"Please run Fusion ETL to generate the TC fusion ready data for {year} data.\"\r\n",
					"    )\r\n",
					"\r\n",
					"# get list of countries in both GWI and TC\r\n",
					"\r\n",
					"# get list of countries in both GWI and TC\r\n",
					"all_countries = list(set(gwi_countries) & set(tc_countries))\r\n",
					"\r\n",
					"for cont in all_countries:\r\n",
					"\r\n",
					"    if check_file_exist('Full_merge/' + f'GWI_TC_{year}_{cont}_full_merged_onehot.parquet'):\r\n",
					"        pass\r\n",
					"\r\n",
					"    else:\r\n",
					"        \r\n",
					"        # NOTE: THIS NEEDS TO BE CHANGED TO ACCOUNT FOR DIFFERENT MODELS\r\n",
					"        filename ='Fusion_models/GWI_TC_'+year+'_'+cont+'_fusion_GowerDist_KNNModel_k1_Choice.parquet'\r\n",
					"        merged_df = pd.read_parquet('abfss://fb-synapse-demo-cr@fbsynapsedemodl.dfs.core.windows.net/'+filename\r\n",
					"    ,storage_options={'account_key':'2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='})\r\n",
					"\r\n",
					"        all_qids = combined_labels['QUID'][combined_labels['Study'] == 'GWI'].tolist() + ['hash', 'core.weighting', 'gwi-ext.weighting']\r\n",
					"\r\n",
					"        quarters = ['Q1' + year[-2:], 'Q2' + year[-2:], 'Q3' + year[-2:], 'Q4' + year[-2:]]\r\n",
					"\r\n",
					"        gwi_quarter_merged_dict = {}\r\n",
					"\r\n",
					"        for gwi_quarter in quarters:\r\n",
					"            gwi_data_dir = 'MWG/'+gwi_quarter+'/MWG/'\r\n",
					"            gwi_data_fn = \"gwi_core_ext_rld_\"+gwi_quarter+\".csv\"\r\n",
					"            account_name = 'fbsynapsedemodl'\r\n",
					"            account_url = 'https://fbsynapsedemodl.dfs.core.windows.net'\r\n",
					"            \r\n",
					"\r\n",
					"            file_system_name = 'fb-synapse-demo-cr'\r\n",
					"            file_path = gwi_data_dir+ gwi_data_fn\r\n",
					"            storage_account_key = '2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='\r\n",
					"\r\n",
					"            file_client = DataLakeFileClient(account_name=account_name,\r\n",
					"                                        account_url=account_url,\r\n",
					"                                        file_system_name=file_system_name,\r\n",
					"                                        file_path=file_path,\r\n",
					"                                        credential=storage_account_key)\r\n",
					"\r\n",
					"        # Download the file from the data lake\r\n",
					"            with open('temp.csv', 'wb') as file:\r\n",
					"                file_client.download_file().readinto(file)\r\n",
					"\r\n",
					"            # Read the CSV columns from the downloaded file\r\n",
					"            with open('temp.csv', 'r') as file:\r\n",
					"                all_cols_in_data = next(csv.reader(file))\r\n",
					"            \r\n",
					"\r\n",
					"            cols_in_data = list(set(all_cols_in_data).intersection(all_qids))\r\n",
					"\r\n",
					"            gwi_all_quarter_hash = Data(source=CSVSource(gwi_data_dir,gwi_data_fn)).run(usecols=['hash'])\r\n",
					"\r\n",
					"            df_merged_quarter_hash = merged_df['hash'].loc[merged_df['wave'] == gwi_quarter[:2].lower() + '_' + year].tolist()\r\n",
					"            print(df_merged_quarter_hash)\r\n",
					"\r\n",
					"            hash_to_skip = list(np.setdiff1d(gwi_all_quarter_hash['hash'].tolist(), df_merged_quarter_hash))\r\n",
					"\r\n",
					"            rows_to_skip = gwi_all_quarter_hash.loc[gwi_all_quarter_hash['hash'].isin(hash_to_skip)].index.to_list()\r\n",
					"            rows_to_skip = rows_to_skip + np.ones(len(rows_to_skip))\r\n",
					"\r\n",
					"            fusion_merge = FusionMerge(tc_vec=tc_onehot_df,\r\n",
					"                                       gwi_vec=Data(source=CSVSource(gwi_data_dir, gwi_data_fn)).run(skiprows=rows_to_skip, usecols=cols_in_data),\r\n",
					"                                       merged_df=merged_df,\r\n",
					"                                       config=tc_config_dict\r\n",
					"                                       )\r\n",
					"\r\n",
					"            gwi_quarter_merged_dict[gwi_quarter] = fusion_merge.run()\r\n",
					"\r\n",
					"        full_onehot_merge = pd.concat(gwi_quarter_merged_dict.values(), ignore_index=True)\r\n",
					"\r\n",
					"        save_df(full_onehot_merge,  'Full_merge/', f'GWI_TC_{year}_{cont}_full_merged_onehot', file_type='parquet')\r\n",
					"        print(f'GWI TC {year} {cont} full merge onehot saved!')"
				]
			}
		]
	}
}