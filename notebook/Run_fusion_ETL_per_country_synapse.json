{
	"name": "Run_fusion_ETL_per_country_synapse",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "fbsynapsedemosp",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3467bce7-e2e0-4ce7-908b-0e28d0dae2e7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/d6240a3c-34a4-4a5c-955b-06228bf34ca8/resourceGroups/fb-synapse-demo-rg/providers/Microsoft.Synapse/workspaces/fb-synapse-demo/bigDataPools/fbsynapsedemosp",
				"name": "fbsynapsedemosp",
				"type": "Spark",
				"endpoint": "https://fb-synapse-demo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/fbsynapsedemosp",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\r\n",
					"\r\n",
					"\r\n",
					"import csv  # noqa: F401\r\n",
					"import json\r\n",
					"import os\r\n",
					"import sys\r\n",
					"from pathlib import Path\r\n",
					"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
					"\r\n",
					"\r\n",
					"import numpy as np  # noqa: F401\r\n",
					"import pandas as pd  # noqa: F401\r\n",
					"from src.transforms.fusion_models_synapse import Fusion, GowerDist, KNNModel  # noqa: F401\r\n",
					"from src.transforms.fusion_utils import (CSVSource, Data, ParquetSource,  # noqa: F401\r\n",
					"                          XLSXSource, do_pandas_test)\r\n",
					"from src.transforms.transforms_synapse import GWIDataTransform  # noqa: F401\r\n",
					"from src.transforms.transforms_synapse import TCDataTransform, save_df\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"if (os.path.exists('/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/src/.env')):\r\n",
					"    print(\"..enter\")\r\n",
					"    with open('/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/src/.env') as f:\r\n",
					"        for line in f:\r\n",
					"            if line.startswith('#') or not line.strip():\r\n",
					"                continue\r\n",
					"            key, value = line.strip().split('=', 1)\r\n",
					"            os.environ[key] = value\r\n",
					"\r\n",
					"environment = os.environ.get('ENVIRONMENT')\r\n",
					"print(\"........................environment\", environment)\r\n",
					"\r\n",
					"container_name=\"fb-synapse-demo-cr\"\r\n",
					"container_conn_str = \"DefaultEndpointsProtocol=https;AccountName=fbsynapsedemodl;AccountKey=2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q==;EndpointSuffix=core.windows.net\"\r\n",
					"\r\n",
					"# print('..........',sys.argv[1])\r\n",
					"\r\n",
					"def json_reader(filename):\r\n",
					"    print('json_file', filename)\r\n",
					"   \r\n",
					"    blob_service_client = BlobServiceClient.from_connection_string(container_conn_str)\r\n",
					"    container_client = blob_service_client.get_container_client(container_name)\r\n",
					"    blob_client = container_client.get_blob_client(filename)\r\n",
					"    streamdownloader = blob_client.download_blob()\r\n",
					"    fileReader = json.loads(streamdownloader.readall())\r\n",
					"    return (fileReader)\r\n",
					"\r\n",
					"def check_file_exist(filename):\r\n",
					"    blob_service_client = BlobServiceClient.from_connection_string(container_conn_str)\r\n",
					"    container_client = blob_service_client.get_container_client(container_name)\r\n",
					"\r\n",
					"    # Check if the blob (file) exists\r\n",
					"    blob_exists = container_client.get_blob_client(filename).exists()\r\n",
					"\r\n",
					"    if blob_exists:\r\n",
					"        return True\r\n",
					"    else:\r\n",
					"        return False\r\n",
					"    \r\n",
					"\r\n",
					"def read_parque(filename):\r\n",
					"    \r\n",
					"    print(\"parquet file\",filename)\r\n",
					"    df = pd.read_parquet('abfss://fb-synapse-demo-cr@fbsynapsedemodl.dfs.core.windows.net/'+filename\r\n",
					"    ,storage_options={'account_key':'2zHvSlf2Y9JZ8DAFpbO8Qfiy2WYTnW6+u7B2SKxVDs7xQYXKAQK2XfaDsvPcG0+yxolU3V/BBndq+AStF/Cs3Q=='})\r\n",
					"    return(df)\r\n",
					"\r\n",
					"\r\n",
					"year = '2022'\r\n",
					"# year = sys.argv[1]\r\n",
					"\r\n",
					"print(f\"Running TC GWI fusion ETL for {year} data.\")\r\n",
					"\r\n",
					"\r\n",
					"tc_data_dir = \"Truth Central parquet files/\"\r\n",
					"year = '2022'\r\n",
					"if year == '2022':\r\n",
					"    tc_resp_fn = 'AllRespondents.parquet'\r\n",
					"    tc_config = 'transforms/TC_2022_lists_and_dicts.json'\r\n",
					"    print(\"tc_config\", tc_config)\r\n",
					"else:\r\n",
					"    raise ValueError(\r\n",
					"        f\"You entered year={year}, which is an invalid option.  Please enter year='2022'.\"\r\n",
					"    )\r\n",
					"\r\n",
					"gwi_config = ('/transforms/GWI_lists_and_dicts_synapse.json')\r\n",
					"\r\n",
					"gwi_config_dict = json_reader(gwi_config)\r\n",
					"\r\n",
					"tc_config_dict = json_reader(tc_config)\r\n",
					"\r\n",
					"if check_file_exist('Fusion_ready/GWI_'+year+'_fusion_cols_ready_for_fusion_synapse.parquet'):\r\n",
					"    df_gwi_fusion_ready = read_parque('Fusion_ready/GWI_'+year+'_fusion_cols_ready_for_fusion_synapse.parquet')\r\n",
					"    print('GWI'+year+'fusion ready loaded')\r\n",
					"else:\r\n",
					"    quarters = ['Q1' + year[-2:], 'Q2' + year[-2:], 'Q3' + year[-2:], 'Q4' + year[-2:]]\r\n",
					"\r\n",
					"    gwi_quarter_dict = {}\r\n",
					"\r\n",
					"    # Load GWI data for the full year.  If data doesn not exist, run transformation to get the quarter data and save it.\r\n",
					"    for gwi_quarter in quarters:\r\n",
					"    \r\n",
					"        if check_file_exist('Fusion_ready/' + f'GWI_{gwi_quarter}_fusion_cols_ready_for_fusion_synapse.parquet'):\r\n",
					"            df_gwi_fusion_ready = read_parque('Fusion_ready/' + f'GWI_{gwi_quarter}_fusion_cols_ready_for_fusion_synapse.parquet')\r\n",
					"            print(f'GWI 2022 fusion ready loaded')\r\n",
					"\r\n",
					"        else:\r\n",
					"            gwi_data_dir = 'MWG/'+gwi_quarter+'/MWG/'\r\n",
					"\r\n",
					"            gwi_data_fn = f\"gwi_core_ext_rld_{gwi_quarter}.csv\"\r\n",
					"            gwi_labels_fn = f'gwi_core_ext_rld_{gwi_quarter}_labels.csv'\r\n",
					"\r\n",
					"            df_labels = Data(source=CSVSource(gwi_data_dir, gwi_labels_fn)).run()\r\n",
					"            print(\"df_labels\",df_labels)\r\n",
					"            GWI_DataTransform = GWIDataTransform(data=Data(\r\n",
					"                source=CSVSource(\r\n",
					"                    gwi_data_dir, \r\n",
					"                    gwi_data_fn,\r\n",
					"                socio_cols_only=df_labels['Question ID'].loc[df_labels['Question'].isin(\r\n",
					"                list(gwi_config_dict['GWI_main_sociodemos'].values()))].tolist())).run(),\r\n",
					"                labels=df_labels,\r\n",
					"                config=gwi_config_dict\r\n",
					"            )\r\n",
					"\r\n",
					"            gwi_quarter_dict[gwi_quarter] = GWI_DataTransform.run()\r\n",
					"            print(\"gwi_quarter_dict\", gwi_quarter_dict)\r\n",
					"            save_df(gwi_quarter_dict[gwi_quarter], 'Fusion_ready/', f'GWI_{gwi_quarter}_fusion_cols_ready_for_fusion', file_type='parquet')\r\n",
					"            print(f'GWI {gwi_quarter} fusion ready saved')\r\n",
					"\r\n",
					"    # Merge the quarters together\r\n",
					"    df_gwi_fusion_ready = pd.concat(gwi_quarter_dict.values(), ignore_index=True)\r\n",
					"\r\n",
					"    save_df(df_gwi_fusion_ready,  'Fusion_ready/', f'GWI_{year}_fusion_cols_ready_for_fusion_synapse', file_type='parquet')\r\n",
					"    print(f'GWI {year} fusion ready saved')\r\n",
					"\r\n",
					"if check_file_exist('Fusion_ready/' + f'TC_{year}_fusion_cols_ready_for_fusion.parquet'):\r\n",
					"    df_tc_fusion_ready = read_parque('Fusion_ready/' + f'TC_{year}_fusion_cols_ready_for_fusion.parquet')\r\n",
					"    print(f'TC {year} fusion ready loaded')\r\n",
					"\r\n",
					"else:\r\n",
					"    TC_DataTransform = TCDataTransform(\r\n",
					"        data=Data(\r\n",
					"            source=ParquetSource(tc_data_dir, tc_resp_fn)).run(),\r\n",
					"        config=tc_config_dict\r\n",
					"    )\r\n",
					"\r\n",
					"    df_tc_fusion_ready = TC_DataTransform.run()\r\n",
					"\r\n",
					"    # check if dtype of \"Age\" is object, if so change it to int64\r\n",
					"    if df_tc_fusion_ready['Age'].dtype == 'object':\r\n",
					"        df_tc_fusion_ready['Age'] = df_tc_fusion_ready['Age'].astype('int64')\r\n",
					"\r\n",
					"    save_df(df_tc_fusion_ready, 'Fusion_ready/', f'TC_{year}_fusion_cols_ready_for_fusion', file_type='parquet')\r\n",
					"    print(f'TC {year} fusion ready saved')\r\n",
					"\r\n",
					"# Get mutual countries between GWI and TC for the year\r\n",
					"gwi_countries = df_gwi_fusion_ready['Country'].unique()\r\n",
					"tc_countries = df_tc_fusion_ready['Country'].unique()\r\n",
					"\r\n",
					"all_countries = [c for c in gwi_countries if c in tc_countries]\r\n",
					"\r\n",
					"for cont in all_countries:\r\n",
					"\r\n",
					"    gwi_temp = df_gwi_fusion_ready.loc[df_gwi_fusion_ready['Country'] == cont].copy()\r\n",
					"    gwi_temp.reset_index(drop=True, inplace=True)\r\n",
					"\r\n",
					"    tc_temp = df_tc_fusion_ready.loc[df_tc_fusion_ready['Country'] == cont].copy()\r\n",
					"    tc_temp.reset_index(drop=True, inplace=True)\r\n",
					"\r\n",
					"    fusion_temp = Fusion(receiver=gwi_temp,\r\n",
					"                         donor=tc_temp,\r\n",
					"                         model=KNNModel(k=1),\r\n",
					"                         fusion_cols=gwi_config_dict['fusion_cols'],\r\n",
					"                         dist_type=GowerDist(num_cols=['Age']),\r\n",
					"                         parallel=False,\r\n",
					"                         )\r\n",
					"\r\n",
					"    fn_prefix = f'GWI_TC_{year}_{cont}_fusion_'\r\n",
					"    fn_params = fusion_temp.get_filename_from_fusion_params()\r\n",
					"\r\n",
					"    if check_file_exist('Fusion_models/' + fn_prefix + fn_params + '.parquet'):\r\n",
					"        print(\"file Exist\")\r\n",
					"        pass\r\n",
					"\r\n",
					"    else:\r\n",
					"        df_cont = fusion_temp.run()\r\n",
					"\r\n",
					"        # Saves fused mutual columns and hash/uuids\r\n",
					"        save_df(df_cont,  'Fusion_models/', fn_prefix + fn_params, file_type='parquet')\r\n",
					"        print(f'Fusion model for {year} {cont} saved!')"
				]
			}
		]
	}
}